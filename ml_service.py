#!/usr/bin/env python3
"""
ML Service - Enhanced Flask API v·ªõi Performance Optimization
Ch·∫°y b·∫±ng: python ml_service.py ho·∫∑c gunicorn ml_service:app
Production ready v·ªõi caching, monitoring, v√† performance optimization
"""

import joblib
import json
import numpy as np
from flask import Flask, request, jsonify
from datetime import datetime, timedelta
import logging
import os
import sys
import time
import threading
from functools import wraps, lru_cache
import hashlib
from collections import defaultdict, deque
import psutil

# Machine Learning imports
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
import pandas as pd

# Performance monitoring imports
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    print("Redis not available - using in-memory cache")

# Configure enhanced logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/var/log/mccva/mccva-ml.log') if os.path.exists('/var/log/mccva') else logging.StreamHandler(),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# Global variables ƒë·ªÉ l∆∞u models
svm_model = None
kmeans_model = None
svm_scaler = None
kmeans_scaler = None
svm_label_encoder = None

# Meta-Learning global variables
meta_learning_model = None
meta_learning_scaler = None
meta_learning_encoder = None
meta_learning_features = None
meta_learning_info = None

# Performance monitoring variables
prediction_cache = {}
request_metrics = defaultdict(list)
performance_stats = {
    'total_requests': 0,
    'total_predictions': 0,
    'cache_hits': 0,
    'cache_misses': 0,
    'average_response_time': 0,
    'error_count': 0,
    'last_reset': datetime.now()
}

# Request rate limiting
request_history = deque(maxlen=1000)  # Keep last 1000 requests

class PerformanceMonitor:
    """Performance monitoring class"""
    
    def __init__(self):
        self.start_time = time.time()
        self.request_count = 0
        self.response_times = deque(maxlen=100)  # Keep last 100 response times
        
    def record_request(self, response_time, endpoint, success=True):
        """Record request metrics"""
        self.request_count += 1
        self.response_times.append(response_time)
        
        request_metrics[endpoint].append({
            'timestamp': datetime.now(),
            'response_time': response_time,
            'success': success
        })
        
        # Keep only last 100 requests per endpoint
        if len(request_metrics[endpoint]) > 100:
            request_metrics[endpoint] = request_metrics[endpoint][-100:]
    
    def get_stats(self):
        """Get performance statistics"""
        avg_response_time = np.mean(self.response_times) if self.response_times else 0
        uptime = time.time() - self.start_time
        
        return {
            'uptime_seconds': uptime,
            'total_requests': self.request_count,
            'average_response_time_ms': avg_response_time * 1000,
            'requests_per_second': self.request_count / uptime if uptime > 0 else 0,
            'memory_usage_mb': psutil.Process().memory_info().rss / 1024 / 1024,
            'cpu_percent': psutil.Process().cpu_percent()
        }

# Initialize performance monitor
perf_monitor = PerformanceMonitor()

def performance_tracker(f):
    """Decorator to track API performance"""
    @wraps(f)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        endpoint = request.endpoint
        
        try:
            # Update global stats
            performance_stats['total_requests'] += 1
            
            result = f(*args, **kwargs)
            response_time = time.time() - start_time
            
            # Record successful request
            perf_monitor.record_request(response_time, endpoint, True)
            
            # Add performance headers
            if hasattr(result, 'headers'):
                result.headers['X-Response-Time'] = f"{response_time:.3f}s"
                result.headers['X-Request-ID'] = str(hash(f"{endpoint}-{start_time}"))
            
            return result
            
        except Exception as e:
            response_time = time.time() - start_time
            performance_stats['error_count'] += 1
            perf_monitor.record_request(response_time, endpoint, False)
            logger.error(f"Error in {endpoint}: {e}, Response time: {response_time:.3f}s")
            raise
    
    return wrapper

def cache_prediction(f):
    """Decorator to cache predictions"""
    @wraps(f)
    def wrapper(*args, **kwargs):
        # Generate cache key from request data
        if request.is_json:
            cache_key = hashlib.md5(
                json.dumps(request.get_json(), sort_keys=True).encode()
            ).hexdigest()
            
            # Check cache
            if cache_key in prediction_cache:
                cache_entry = prediction_cache[cache_key]
                
                # Check if cache is still valid (5 minutes)
                if datetime.now() - cache_entry['timestamp'] < timedelta(minutes=5):
                    performance_stats['cache_hits'] += 1
                    logger.info(f"Cache hit for {request.endpoint}")
                    
                    # Add cache headers
                    response_data = cache_entry['data']
                    response_data['cached'] = True
                    response_data['cache_timestamp'] = cache_entry['timestamp'].isoformat()
                    
                    return jsonify(response_data)
            
            # Cache miss - execute function
            performance_stats['cache_misses'] += 1
            result = f(*args, **kwargs)
            
            # Cache the result
            if hasattr(result, 'get_json'):
                prediction_cache[cache_key] = {
                    'data': result.get_json(),
                    'timestamp': datetime.now()
                }
                
                # Cleanup old cache entries (keep last 1000)
                if len(prediction_cache) > 1000:
                    oldest_key = min(prediction_cache.keys(), 
                                   key=lambda k: prediction_cache[k]['timestamp'])
                    del prediction_cache[oldest_key]
            
            return result
        else:
            return f(*args, **kwargs)
    
    return wrapper

@lru_cache(maxsize=128)
def get_feature_hash(features_tuple):
    """Cache feature preprocessing"""
    return hashlib.md5(str(features_tuple).encode()).hexdigest()

def load_models():
    """Load c√°c m√¥ h√¨nh khi kh·ªüi ƒë·ªông service"""
    global svm_model, kmeans_model, svm_scaler, kmeans_scaler, svm_label_encoder
    global meta_learning_model, meta_learning_scaler, meta_learning_encoder, meta_learning_features, meta_learning_info
    
    try:
        # ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ƒë√∫ng
        base_dir = os.getcwd()
        models_dir = os.path.join(base_dir, "models")
        
        logger.info(f"ƒêang load models t·ª´ {models_dir}...")
        
        # H√†m load model v·ªõi h·ªó tr·ª£ pickle v√† joblib
        def load_model(model_path):
            try:
                # Th·ª≠ load v·ªõi joblib tr∆∞·ªõc
                import joblib
                return joblib.load(model_path)
            except Exception as e:
                logger.warning(f"Kh√¥ng th·ªÉ load model b·∫±ng joblib ({e}), ƒëang th·ª≠ v·ªõi pickle...")
                try:
                    # Th·ª≠ v·ªõi pickle n·∫øu joblib th·∫•t b·∫°i
                    import pickle
                    with open(model_path, 'rb') as f:
                        return pickle.load(f)
                except Exception as e2:
                    logger.error(f"Kh√¥ng th·ªÉ load model: {model_path}, l·ªói: {e2}")
                    raise e2
        
        try:
            # Th·ª≠ load models th·∫≠t
            logger.info("ƒêang load m√¥ h√¨nh SVM...")
            svm_model = load_model(os.path.join(models_dir, "svm_model.joblib"))
            svm_scaler = load_model(os.path.join(models_dir, "svm_scaler.joblib"))
            svm_label_encoder = load_model(os.path.join(models_dir, "svm_label_encoder.joblib"))
            
            logger.info("ƒêang load m√¥ h√¨nh K-Means...")
            kmeans_model = load_model(os.path.join(models_dir, "kmeans_model.joblib"))
            kmeans_scaler = load_model(os.path.join(models_dir, "kmeans_scaler.joblib"))
            
            logger.info("ƒêang load m√¥ h√¨nh Meta-Learning...")
            meta_learning_model = load_model(os.path.join(models_dir, "meta_learning_model.joblib"))
            meta_learning_scaler = load_model(os.path.join(models_dir, "meta_learning_scaler.joblib"))
            meta_learning_encoder = load_model(os.path.join(models_dir, "meta_learning_encoder.joblib"))
            meta_learning_features = load_model(os.path.join(models_dir, "meta_learning_features.joblib"))
            meta_learning_info = load_model(os.path.join(models_dir, "meta_learning_info.joblib"))
        
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ load models th·∫≠t, ƒëang t·∫°o simple models thay th·∫ø: {str(e)}")
            
            # Import simple models n·∫øu kh√¥ng th·ªÉ load models th·∫≠t
            try:
                # Th·ª≠ import t·ª´ file
                from simple_models import SimpleSVM, SimpleKMeans, SimpleMetaLearning, SimpleScaler, SimpleLabelEncoder
                
                logger.info("T·∫°o c√°c m√¥ h√¨nh ƒë∆°n gi·∫£n t·ª´ simple_models.py...")
                svm_model = SimpleSVM()
                svm_scaler = SimpleScaler()
                svm_label_encoder = SimpleLabelEncoder()
                
                kmeans_model = SimpleKMeans()
                kmeans_scaler = SimpleScaler()
                
                meta_learning_model = SimpleMetaLearning()
                meta_learning_scaler = SimpleScaler()
                meta_learning_encoder = SimpleLabelEncoder()
                meta_learning_features = ['cpu', 'memory', 'storage', 'network', 'priority', 
                                        'svm_small', 'svm_medium', 'svm_large',
                                        'cluster_0', 'cluster_1', 'cluster_2']
                meta_learning_info = {
                    'architecture': 'Simple Model for Demo',
                    'test_accuracy': 0.6,  # Gi·∫£m ƒë·ªô ch√≠nh x√°c ƒë·ªÉ ph·∫£n √°nh th·ª±c t·∫ø
                    'n_features': 15
                }
            
                # N·∫øu kh√¥ng c√≥ file, t·∫°o c√°c models trong b·ªô nh·ªõ
            except ImportError:
                logger.info("File simple_models.py kh√¥ng t·ªìn t·∫°i, t·∫°o models trong memory...")
                
                # Simple SVM
                class SimpleSVM:
                    def __init__(self):
                        self.classes_ = np.array(['small', 'medium', 'large'])
                        self.n_support_ = np.array([10, 10, 10])
                        self.kernel = 'linear'  # Gi·ªëng v·ªõi m√¥ h√¨nh th·∫≠t
                    
                    def predict(self, X):
                        result = np.array(['small'] * X.shape[0])
                        result[X[:, 0] > 0.3] = 'medium'
                        result[X[:, 0] > 0.7] = 'large'
                        return result
                    
                    def predict_proba(self, X):
                        n = X.shape[0]
                        result = np.zeros((n, 3))
                        for i in range(n):
                            if X[i, 0] <= 0.3:
                                result[i] = [0.8, 0.15, 0.05]
                            elif X[i, 0] <= 0.7:
                                result[i] = [0.15, 0.7, 0.15]
                            else:
                                result[i] = [0.05, 0.15, 0.8]
                        return result
                
                # Simple KMeans
                class SimpleKMeans:
                    def __init__(self):
                        self.n_clusters = 3  # ƒê√£ s·ª≠a t·ª´ 5 th√†nh 3 clusters
                        self.inertia_ = 42.0
                    
                    def predict(self, X):
                        return np.array([i % 3 for i in range(X.shape[0])])  # ƒê√£ s·ª≠a t·ª´ 5 th√†nh 3
                    
                    def transform(self, X):
                        n = X.shape[0]
                        result = np.ones((n, 3)) * 10  # ƒê√£ s·ª≠a t·ª´ 5 th√†nh 3
                        for i in range(n):
                            cluster = i % 3  # ƒê√£ s·ª≠a t·ª´ 5 th√†nh 3
                            result[i, cluster] = 0.1
                        return result
                
                # Simple Meta-Learning
                class SimpleMetaLearning:
                    def __init__(self):
                        self.classes_ = np.array(['small', 'medium', 'large'])
                    
                    def predict(self, X):
                        # Logic ƒë∆°n gi·∫£n d·ª±a tr√™n feature ƒë·∫ßu ti√™n v√† th·ª© hai
                        result = np.array(['medium'] * X.shape[0])
                        
                        # N·∫øu feature 0 (th∆∞·ªùng l√† CPU) th·∫•p -> small
                        result[X[:, 0] < -0.5] = 'small'
                        
                        # N·∫øu feature 0 (th∆∞·ªùng l√† CPU) cao -> large
                        result[X[:, 0] > 0.5] = 'large'
                        
                        # N·∫øu feature 5-7 (th∆∞·ªùng l√† SVM probs) c√≥ gi√° tr·ªã cao cho small/large
                        if X.shape[1] > 7:  # ƒê·∫£m b·∫£o c√≥ ƒë·ªß features
                            # N·∫øu SVM d·ª± ƒëo√°n small v·ªõi ƒë·ªô tin c·∫≠y cao
                            result[X[:, 5] > 0.7] = 'small'
                            
                            # N·∫øu SVM d·ª± ƒëo√°n large v·ªõi ƒë·ªô tin c·∫≠y cao
                            result[X[:, 7] > 0.7] = 'large'
                        
                        return result
                    
                    def predict_proba(self, X):
                        n = X.shape[0]
                        result = np.zeros((n, 3))
                        
                        # Logic t∆∞∆°ng t·ª± nh∆∞ predict nh∆∞ng tr·∫£ v·ªÅ probabilities
                        for i in range(n):
                            # Default medium
                            probs = [0.2, 0.6, 0.2]
                            
                            # ƒêi·ªÅu ch·ªânh d·ª±a tr√™n feature 0
                            if X[i, 0] < -0.5:
                                probs = [0.7, 0.2, 0.1]  # small
                            elif X[i, 0] > 0.5:
                                probs = [0.1, 0.2, 0.7]  # large
                                
                            # ƒêi·ªÅu ch·ªânh d·ª±a tr√™n SVM probs n·∫øu c√≥
                            if X.shape[1] > 7:
                                if X[i, 5] > 0.7:  # SVM small
                                    probs = [0.8, 0.15, 0.05]
                                elif X[i, 7] > 0.7:  # SVM large
                                    probs = [0.05, 0.15, 0.8]
                            
                            result[i] = probs
                            
                        return result
                
                # Simple Scaler
                class SimpleScaler:
                    def __init__(self):
                        self.mean_ = np.zeros(9)  # TƒÉng l√™n 9 features ƒë·ªÉ ph√π h·ª£p v·ªõi m√¥ h√¨nh th·∫≠t
                        self.scale_ = np.ones(9)
                    
                    def transform(self, X):
                        if X.shape[1] < 9:
                            padding = np.zeros((X.shape[0], 9 - X.shape[1]))
                            X_padded = np.hstack((X, padding))
                            return X_padded
                        return (X - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-8)
                    
                    def fit_transform(self, X):
                        return self.transform(X)
                
                # Simple Label Encoder
                class SimpleLabelEncoder:
                    def __init__(self):
                        self.classes_ = np.array(['small', 'medium', 'large'])
                    
                    def transform(self, y):
                        result = np.zeros(len(y), dtype=int)
                        for i, val in enumerate(y):
                            if val == 'small':
                                result[i] = 0
                            elif val == 'medium':
                                result[i] = 1
                            else:
                                result[i] = 2
                        return result
                    
                    def fit_transform(self, y):
                        return self.transform(y)
                    
                    def inverse_transform(self, y):
                        result = np.array(['medium'] * len(y))
                        result[y == 0] = 'small'
                        result[y == 2] = 'large'
                        return result
            
            # T·∫°o simple models
            logger.info("T·∫°o c√°c m√¥ h√¨nh ƒë∆°n gi·∫£n cho demo...")
            svm_model = SimpleSVM()
            svm_scaler = SimpleScaler()
            svm_label_encoder = SimpleLabelEncoder()
            
            kmeans_model = SimpleKMeans()
            kmeans_scaler = SimpleScaler()
            
            meta_learning_model = SimpleMetaLearning()
            meta_learning_scaler = SimpleScaler()
            meta_learning_encoder = SimpleLabelEncoder()
            meta_learning_features = ['cpu', 'memory', 'storage', 'network', 'priority', 
                                    'svm_small', 'svm_medium', 'svm_large',
                                        'cluster_0', 'cluster_1', 'cluster_2']
            meta_learning_info = {
                'architecture': 'Simple Model for Demo',
                    'test_accuracy': 0.6,  # Gi·∫£m ƒë·ªô ch√≠nh x√°c ƒë·ªÉ ph·∫£n √°nh th·ª±c t·∫ø
                    'n_features': 15
            }
            
            logger.info("‚úÖ ƒê√£ t·∫°o simple models thay th·∫ø th√†nh c√¥ng!")
        
        logger.info("‚úÖ T·∫•t c·∫£ m√¥ h√¨nh ƒë√£ s·∫µn s√†ng!")
        logger.info(f"SVM Model: {svm_model.kernel} kernel, {sum(svm_model.n_support_) if hasattr(svm_model, 'n_support_') else 'N/A'} support vectors")
        logger.info(f"K-Means Model: {kmeans_model.n_clusters} clusters")
        logger.info(f"Meta-Learning Model: {meta_learning_info.get('architecture', 'Unknown')} architecture, accuracy: {meta_learning_info.get('test_accuracy', 'Unknown')}")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi load models: {e}")
        logger.error("‚ö†Ô∏è Service s·∫Ω ho·∫°t ƒë·ªông nh∆∞ng kh√¥ng th·ªÉ th·ª±c hi·ªán d·ª± ƒëo√°n!")

# Load models khi kh·ªüi ƒë·ªông with gunicorn
try:
    load_models()
except Exception as e:
    print(f"Failed to load models at startup: {e}")

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    try:
        models_status = {
            "svm": svm_model is not None,
            "kmeans": kmeans_model is not None,
            "svm_scaler": svm_scaler is not None,
            "kmeans_scaler": kmeans_scaler is not None,
            "svm_label_encoder": svm_label_encoder is not None,
            "meta_learning": meta_learning_model is not None,
            "meta_learning_scaler": meta_learning_scaler is not None,
            "meta_learning_encoder": meta_learning_encoder is not None
        }
        
        all_models_loaded = all(models_status.values())
        
        return jsonify({
            "status": "healthy" if all_models_loaded else "degraded",
            "timestamp": datetime.now().isoformat(),
            "models_loaded": models_status,
            "service": "mccva-ml-service",
            "version": "2.0.0",
            "meta_learning_ready": meta_learning_model is not None
        })
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return jsonify({
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/predict/makespan', methods=['POST'])
@performance_tracker
@cache_prediction
def predict_makespan():
    """
    üéØ SVM PREDICTION ENDPOINT
    D·ª± ƒëo√°n makespan (small/medium/large) d·ª±a tr√™n SVM model
    Input: {"features": [CPU, Memory, Storage, Network, Priority]}
    Output: {"makespan": "small|medium|large", "confidence": float, "features": [...], "timestamp": datetime}
    """
    try:
        if not svm_model or not svm_scaler:
            return jsonify({"error": "SVM model not loaded"}), 503
            
        data = request.get_json()
        if not data:
            return jsonify({"error": "No request data"}), 400
            
        # Extract features from request
        features = np.array(data.get('features', [])).reshape(1, -1)
        if features.shape[1] < 5:
            return jsonify({"error": f"Expected at least 5 features, got {features.shape[1]}"}), 400
            
        # Ensure we use only the expected number of features for SVM
        features = features[:, :svm_scaler.mean_.shape[0]]
        
        # Scale features and predict
        features_scaled = svm_scaler.transform(features)
        prediction = svm_model.predict(features_scaled)[0]
        confidence = float(np.max(svm_model.predict_proba(features_scaled)[0]))
        
        result = {
            "makespan": prediction,
            "confidence": confidence,
            "features": features.tolist()[0],
            "timestamp": datetime.now().isoformat()
        }
        
        return jsonify(result)
            
    except Exception as e:
        logger.error(f"SVM prediction error: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/predict/vm_cluster', methods=['POST'])
@performance_tracker
@cache_prediction
def predict_vm_cluster():
    """
    API d·ª± ƒëo√°n c·ª•m cho VM
    Input: {"vm_features": [cpu_usage, ram_usage, storage_usage]}
    Output: {"cluster": int, "distance": float, "centroid": [float, float, float]}
    """
    try:
        if kmeans_model is None or kmeans_scaler is None:
            return jsonify({"error": "K-Means model not loaded"}), 503
        
        data = request.get_json()
        if not data or "vm_features" not in data:
            return jsonify({"error": "Missing 'vm_features' field"}), 400
        
        vm_features = data["vm_features"]
        
        # Validate input
        if len(vm_features) != 3:
            return jsonify({"error": "Expected 3 features: [cpu_usage, ram_usage, storage_usage]"}), 400
        
        # Validate ranges (0-1)
        for i, feature in enumerate(vm_features):
            if not (0 <= feature <= 1):
                return jsonify({"error": f"Feature {i} must be between 0-1 (percentage)"}), 400
        
        # Chu·∫©n h√≥a d·ªØ li·ªáu
        vm_scaled = kmeans_scaler.transform([vm_features])
        
        # D·ª± ƒëo√°n c·ª•m
        cluster = int(kmeans_model.predict(vm_scaled)[0])
        
        # T√≠nh kho·∫£ng c√°ch ƒë·∫øn centroid
        distances = kmeans_model.transform(vm_scaled)[0]
        distance_to_center = float(distances[cluster])
        
        # L·∫•y centroid c·ªßa c·ª•m
        centroid = kmeans_model.cluster_centers_[cluster].tolist()
        
        logger.info(f"VM Cluster: {cluster} (distance: {distance_to_center:.3f}) for features: {vm_features}")
        
        return jsonify({
            "cluster": cluster,
            "distance": distance_to_center,
            "centroid": centroid,
            "vm_features": vm_features,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error in predict_vm_cluster: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/predict/batch', methods=['POST'])
@performance_tracker
@cache_prediction
def predict_batch():
    """
    API d·ª± ƒëo√°n h√†ng lo·∫°t
    Input: {"requests": [[features1], [features2], ...], "vm_usages": [[vm_features1], [vm_features2], ...]}
    Output: {"predictions": [{"makespan": str, "cluster": int, ...}, ...]}
    """
    try:
        data = request.get_json()
        if not data or "requests" not in data or "vm_usages" not in data:
            return jsonify({"error": "Missing 'requests' or 'vm_usages' field"}), 400
        
        requests = data["requests"]
        vm_usages = data["vm_usages"]
        
        if len(requests) != len(vm_usages):
            return jsonify({"error": "Number of requests must match number of VM usages"}), 400
        
        predictions = []
        
        for i, (req_features, vm_features) in enumerate(zip(requests, vm_usages)):
            try:
                # D·ª± ƒëo√°n makespan
                req_scaled = svm_scaler.transform([req_features])
                makespan = svm_model.predict(req_scaled)[0]
                
                # D·ª± ƒëo√°n cluster
                vm_scaled = kmeans_scaler.transform([vm_features])
                cluster = int(kmeans_model.predict(vm_scaled)[0])
                
                predictions.append({
                    "id": i + 1,
                    "makespan": makespan,
                    "cluster": cluster,
                    "request_features": req_features,
                    "vm_features": vm_features
                })
                
            except Exception as e:
                logger.error(f"Error processing item {i}: {e}")
                predictions.append({
                    "id": i + 1,
                    "error": str(e)
                })
        
        logger.info(f"Batch prediction completed: {len(predictions)} items")
        
        return jsonify({
            "predictions": predictions,
            "total": len(predictions),
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error in predict_batch: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/vm_clusters/info', methods=['GET'])
@performance_tracker
@cache_prediction
def get_vm_clusters_info():
    """
    API l·∫•y th√¥ng tin c√°c c·ª•m VM
    Output: {"clusters": [{"id": int, "centroid": [float, float, float], "size": int}, ...]}
    """
    try:
        clusters_info = []
        
        for i in range(kmeans_model.n_clusters):
            centroid = kmeans_model.cluster_centers_[i].tolist()
            # ƒê·∫øm s·ªë VM trong c·ª•m (t·ª´ labels ƒë√£ l∆∞u)
            size = int(np.sum(kmeans_model.labels_ == i))
            
            clusters_info.append({
                "id": i,
                "centroid": centroid,
                "size": size
            })
        
        return jsonify({
            "clusters": clusters_info,
            "total_clusters": kmeans_model.n_clusters,
            "total_vms": len(kmeans_model.labels_),
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error in get_vm_clusters_info: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/models/info', methods=['GET'])
@performance_tracker
@cache_prediction
def get_models_info():
    """
    API l·∫•y th√¥ng tin v·ªÅ c√°c m√¥ h√¨nh
    """
    try:
        svm_info = {
            "kernel": svm_model.kernel,
            "c_parameter": svm_model.C,
            "gamma": svm_model.gamma,
            "support_vectors": int(sum(svm_model.n_support_)) if svm_model else 0,
            "classes": svm_model.classes_.tolist() if svm_model else []
        }
        
        kmeans_info = {
            "n_clusters": kmeans_model.n_clusters,
            "inertia": float(kmeans_model.inertia_) if kmeans_model else 0,
            "n_iter": kmeans_model.n_iter_ if kmeans_model else 0
        }
        
        return jsonify({
            "svm_model": svm_info,
            "kmeans_model": kmeans_info,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error in get_models_info: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/predict/enhanced', methods=['POST'])
@performance_tracker
@cache_prediction
def predict_enhanced():
    """
    üéØ ENHANCED PREDICTION ENDPOINT (SVM + K-MEANS)
    Combines SVM Classification and K-Means Clustering
    Input: {"features": [CPU, Memory, Storage, Network, Priority]}
    Output: {"makespan": "small|medium|large", "cluster": int, "confidence": float}
    """
    try:
        if not all([svm_model, kmeans_model, svm_scaler, kmeans_scaler]):
            return jsonify({"error": "Required models not loaded"}), 503
            
        data = request.get_json()
        if not data:
            return jsonify({"error": "No request data"}), 400
            
        # Extract features
        features = np.array(data.get('features', [])).reshape(1, -1)
        if features.shape[1] < 5:
            return jsonify({"error": f"Expected at least 5 features, got {features.shape[1]}"}), 400
            
        # Ensure correct feature dimensions
        features_svm = features[:, :svm_scaler.mean_.shape[0]]
        features_kmeans = features[:, :kmeans_scaler.mean_.shape[0]]
        
        # SVM prediction
        features_svm_scaled = svm_scaler.transform(features_svm)
        svm_prediction = svm_model.predict(features_svm_scaled)[0]
        svm_confidence = float(np.max(svm_model.predict_proba(features_svm_scaled)[0]))
        
        # K-Means prediction
        features_kmeans_scaled = kmeans_scaler.transform(features_kmeans)
        kmeans_cluster = int(kmeans_model.predict(features_kmeans_scaled)[0])
        kmeans_distances = kmeans_model.transform(features_kmeans_scaled)[0]
        kmeans_distance = float(np.min(kmeans_distances))
        
        # Combined result
        result = {
            "makespan": svm_prediction,
            "cluster": kmeans_cluster,
            "confidence": svm_confidence,
            "cluster_distance": kmeans_distance,
            "features": features.tolist()[0],
            "timestamp": datetime.now().isoformat()
        }
        
        return jsonify(result)
            
    except Exception as e:
        logger.error(f"Enhanced prediction error: {str(e)}")
        return jsonify({"error": str(e)}), 500

def extract_enhanced_features(features):
    """Extract enhanced features t·ª´ basic features"""
    cpu_cores, memory, storage, network_bandwidth, priority = features
    
    enhanced = {
        # Basic features
        "cpu_cores": cpu_cores,
        "memory": memory,
        "storage": storage,
        "network_bandwidth": network_bandwidth,
        "priority": priority,
        
        # Derived features (AI insights)
        "compute_intensity": cpu_cores / memory if memory > 0 else 0,
        "storage_intensity": storage / memory if memory > 0 else 0,
        "network_intensity": network_bandwidth / cpu_cores if cpu_cores > 0 else 0,
        "resource_ratio": (cpu_cores * memory) / storage if storage > 0 else 0,
        
        # Workload classification
        "is_compute_intensive": cpu_cores / memory > 0.5 if memory > 0 else False,
        "is_memory_intensive": memory > 16,
        "is_storage_intensive": storage > 500,
        "is_network_intensive": network_bandwidth > 5000,
        
        # Priority-based features
        "high_priority": priority >= 4,
        "low_priority": priority <= 2,
        "priority_weight": priority / 5.0,
        
        # Resource utilization patterns
        "balanced_resources": abs(cpu_cores - memory/4) < 2,  # CPU and memory balanced
        "storage_heavy": storage > (cpu_cores * memory * 2),
        "network_heavy": network_bandwidth > (cpu_cores * 1000)
    }
    
    return enhanced

def get_rule_based_prediction(enhanced_features):
    """Rule-based prediction d·ª±a tr√™n business logic"""
    score = 0
    confidence = 0.5  # Base confidence
    
    # Rule 1: Compute-intensive workloads
    if enhanced_features["is_compute_intensive"]:
        score += 30
        confidence += 0.1
    
    # Rule 2: High priority requests
    if enhanced_features["high_priority"]:
        score += 25
        confidence += 0.15
    
    # Rule 3: Memory-intensive workloads
    if enhanced_features["is_memory_intensive"]:
        score += 20
        confidence += 0.1
    
    # Rule 4: Network-intensive workloads
    if enhanced_features["is_network_intensive"]:
        score += 15
        confidence += 0.1
    
    # Rule 5: Storage-heavy workloads
    if enhanced_features["storage_heavy"]:
        score += 10
        confidence += 0.05
    
    # Determine makespan based on score
    if score >= 60:
        prediction = "large"
    elif score >= 30:
        prediction = "medium"
    else:
        prediction = "small"
    
    confidence = min(confidence, 0.9)  # Cap confidence at 0.9
    
    return prediction, confidence

# üß† META-LEARNING ENSEMBLE CLASS - Advanced AI-based ensemble
class MetaLearningEnsemble:
    """
    üéØ ADVANCED META-LEARNING ENSEMBLE
    Uses Neural Network to learn optimal combination of SVM + K-Means + Rules
    Instead of hardcoded if-else logic!
    """
    
    def __init__(self):
        self.meta_model = None
        self.meta_scaler = StandardScaler()
        self.meta_label_encoder = LabelEncoder()
        self.is_trained = False
        self.training_data = []
        self.feature_names = [
            'svm_confidence', 'kmeans_confidence', 'rule_confidence',
            'svm_small_score', 'svm_medium_score', 'svm_large_score',
            'cluster_id', 'cluster_distance', 
            'compute_intensity', 'memory_intensity', 'storage_intensity',
            'is_high_priority', 'resource_balance_score'
        ]
        
    def collect_training_sample(self, svm_pred, svm_conf, kmeans_cluster, kmeans_conf, 
                              rule_pred, rule_conf, enhanced_features, true_label=None):
        """
        üéØ Thu th·∫≠p training samples ƒë·ªÉ Meta-Learning t·ª± h·ªçc
        """
        # T·∫°o feature vector cho meta-model
        meta_features = self._extract_meta_features(
            svm_pred, svm_conf, kmeans_cluster, kmeans_conf, 
            rule_pred, rule_conf, enhanced_features
        )
        
        # L∆∞u sample (n·∫øu c√≥ true_label)
        if true_label:
            self.training_data.append({
                'features': meta_features,
                'label': true_label,
                'timestamp': datetime.now()
            })
            
        return meta_features
    
    def _extract_meta_features(self, svm_pred, svm_conf, kmeans_cluster, kmeans_conf, 
                             rule_pred, rule_conf, enhanced_features):
        """
        üîß Tr√≠ch xu·∫•t features cho Meta-Learning model
        """
        # Chuy·ªÉn SVM prediction th√†nh scores
        svm_scores = {'small': 0, 'medium': 0, 'large': 0}
        svm_scores[svm_pred] = 1
        
        # T√≠nh cluster distance (normalized)
        cluster_distance_norm = 1 / (1 + kmeans_conf) if kmeans_conf > 0 else 0.5
        
        # Extract key business features
        compute_intensity = enhanced_features.get('compute_intensity', 0)
        memory_intensity = enhanced_features.get('memory_intensity', 0) 
        storage_intensity = enhanced_features.get('storage_intensity', 0)
        is_high_priority = float(enhanced_features.get('high_priority', False))
        
        # Resource balance score
        balance_score = 1 - abs(compute_intensity - 0.5) - abs(memory_intensity - 0.5)
        
        meta_features = [
            svm_conf, kmeans_conf, rule_conf,  # Base confidences
            svm_scores['small'], svm_scores['medium'], svm_scores['large'],  # SVM outputs
            float(kmeans_cluster), cluster_distance_norm,  # K-Means outputs
            compute_intensity, memory_intensity, storage_intensity,  # Resource patterns
            is_high_priority, balance_score  # Business logic
        ]
        
        return meta_features
    
    def train_meta_model(self, min_samples=100):
        """
        üöÄ Train Meta-Learning Neural Network
        """
        if len(self.training_data) < min_samples:
            logger.warning(f"Not enough training data: {len(self.training_data)} < {min_samples}")
            return False
            
        try:
            # Prepare training data
            X = np.array([sample['features'] for sample in self.training_data])
            y = [sample['label'] for sample in self.training_data]
            
            # Encode labels
            y_encoded = self.meta_label_encoder.fit_transform(y)
            
            # Scale features
            X_scaled = self.meta_scaler.fit_transform(X)
            
            # Train Neural Network Meta-Model
            self.meta_model = MLPClassifier(
                hidden_layer_sizes=(64, 32, 16),  # 3-layer deep network
                activation='relu',
                solver='adam',
                alpha=0.001,
                learning_rate='adaptive',
                max_iter=500,
                random_state=42,
                early_stopping=True,
                validation_fraction=0.2
            )
            
            # Split for validation
            X_train, X_val, y_train, y_val = train_test_split(
                X_scaled, y_encoded, test_size=0.2, random_state=42
            )
            
            # Train model
            self.meta_model.fit(X_train, y_train)
            
            # Validate
            val_accuracy = self.meta_model.score(X_val, y_val)
            train_accuracy = self.meta_model.score(X_train, y_train)
            
            self.is_trained = True
            
            logger.info(f"Meta-Learning Model Trained Successfully!")
            logger.info(f"Training Accuracy: {train_accuracy:.3f}")
            logger.info(f"Validation Accuracy: {val_accuracy:.3f}")
            logger.info(f"Training Samples: {len(self.training_data)}")
            
            return True
            
        except Exception as e:
            logger.error(f"Error training meta-model: {e}")
            return False
    
    def predict(self, svm_pred, svm_conf, kmeans_cluster, kmeans_conf, 
                rule_pred, rule_conf, enhanced_features):
        """
        üéØ Meta-Learning Prediction - AI learns the best combination!
        """
        try:
            # Extract meta-features
            meta_features = self._extract_meta_features(
                svm_pred, svm_conf, kmeans_cluster, kmeans_conf,
                rule_pred, rule_conf, enhanced_features
            )
            
            if self.is_trained and self.meta_model is not None:
                # AI-POWERED ENSEMBLE PREDICTION
                meta_features_scaled = self.meta_scaler.transform([meta_features])
                
                # Get prediction probabilities
                prediction_proba = self.meta_model.predict_proba(meta_features_scaled)[0]
                predicted_class_idx = np.argmax(prediction_proba)
                predicted_class = self.meta_label_encoder.inverse_transform([predicted_class_idx])[0]
                
                # Confidence from neural network
                ensemble_confidence = float(np.max(prediction_proba))
                
                # Get feature importance (approximate)
                feature_importance = abs(meta_features_scaled[0])
                feature_importance = feature_importance / np.sum(feature_importance)
                
                return {
                    "makespan": predicted_class,
                    "cluster": kmeans_cluster,
                    "confidence": ensemble_confidence,
                    "method": "MetaLearning_NeuralNetwork",
                    "model_contributions": {
                        "svm_influence": float(feature_importance[0:3].mean()),  # SVM features
                        "kmeans_influence": float(feature_importance[6:8].mean()),  # K-Means features
                        "business_influence": float(feature_importance[8:].mean())  # Business features
                    },
                    "prediction_probabilities": {
                        class_name: float(prob) 
                        for class_name, prob in zip(
                            self.meta_label_encoder.classes_, 
                            prediction_proba
                        )
                    }
                }
            else:
                # üìä FALLBACK: Intelligent weighted voting (better than if-else)
                return self._intelligent_fallback(
                    svm_pred, svm_conf, kmeans_cluster, kmeans_conf,
                    rule_pred, rule_conf, enhanced_features
                )
                
        except Exception as e:
            logger.error(f"Error in meta-learning prediction: {e}")
            # Fallback to simple combination
            return self._intelligent_fallback(
                svm_pred, svm_conf, kmeans_cluster, kmeans_conf,
                rule_pred, rule_conf, enhanced_features
            )
    
    def _intelligent_fallback(self, svm_pred, svm_conf, kmeans_cluster, kmeans_conf,
                            rule_pred, rule_conf, enhanced_features):
        """
        üîÑ Intelligent fallback with adaptive weighting (no hardcoded if-else!)
        """
        # Adaptive weights based on model confidence and domain knowledge
        base_weights = {'svm': 0.4, 'kmeans': 0.3, 'rule': 0.3}
        
        # Confidence-based adjustment
        total_confidence = svm_conf + kmeans_conf + rule_conf
        if total_confidence > 0:
            conf_weights = {
                'svm': svm_conf / total_confidence,
                'kmeans': kmeans_conf / total_confidence, 
                'rule': rule_conf / total_confidence
            }
            
            # Combine base and confidence weights
            final_weights = {
                'svm': (base_weights['svm'] + conf_weights['svm']) / 2,
                'kmeans': (base_weights['kmeans'] + conf_weights['kmeans']) / 2,
                'rule': (base_weights['rule'] + conf_weights['rule']) / 2
            }
        else:
            final_weights = base_weights
        
        # Soft voting instead of hard if-else
        makespan_scores = {"small": 0, "medium": 0, "large": 0}
        
        # SVM contribution
        makespan_scores[svm_pred] += final_weights['svm']
        
        # Rule-based contribution  
        makespan_scores[rule_pred] += final_weights['rule']
        
        # K-Means contribution with soft mapping
        cluster_to_workload = {
            0: {"small": 0.7, "medium": 0.3, "large": 0.0},
            1: {"small": 0.5, "medium": 0.4, "large": 0.1},
            2: {"small": 0.3, "medium": 0.6, "large": 0.1},
            3: {"small": 0.1, "medium": 0.7, "large": 0.2},
            4: {"small": 0.0, "medium": 0.5, "large": 0.5},
            5: {"small": 0.0, "medium": 0.3, "large": 0.7}
        }
        
        cluster_mapping = cluster_to_workload.get(kmeans_cluster, 
                                                {"small": 0.33, "medium": 0.33, "large": 0.33})
        
        for workload, prob in cluster_mapping.items():
            makespan_scores[workload] += final_weights['kmeans'] * prob
        
        # Final decision
        final_makespan = max(makespan_scores, key=makespan_scores.get)
        
        # Calculate ensemble confidence
        ensemble_confidence = max(makespan_scores.values()) / sum(makespan_scores.values())
        
        return {
            "makespan": final_makespan,
            "cluster": kmeans_cluster,
            "confidence": ensemble_confidence,
            "method": "IntelligentFallback_SoftVoting",
            "weights": final_weights,
            "makespan_scores": makespan_scores
        }
    
    def save_model(self, filepath="models/meta_ensemble.pkl"):
        """üíæ Save trained meta-model"""
        if self.is_trained:
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            joblib.dump({
                'meta_model': self.meta_model,
                'meta_scaler': self.meta_scaler,
                'meta_label_encoder': self.meta_label_encoder,
                'is_trained': self.is_trained,
                'training_samples': len(self.training_data)
            }, filepath)
            logger.info(f"Meta-ensemble model saved to {filepath}")
    
    def load_model(self, filepath="models/meta_ensemble.pkl"):
        """üìÇ Load pre-trained meta-model"""
        if os.path.exists(filepath):
            try:
                data = joblib.load(filepath)
                self.meta_model = data['meta_model']
                self.meta_scaler = data['meta_scaler'] 
                self.meta_label_encoder = data['meta_label_encoder']
                self.is_trained = data['is_trained']
                logger.info(f"Meta-ensemble model loaded from {filepath}")
                logger.info(f"Training samples: {data.get('training_samples', 'unknown')}")
                return True
            except Exception as e:
                logger.error(f"Error loading meta-model: {e}")
                return False
        return False

# Initialize Meta-Learning Ensemble
meta_ensemble = MetaLearningEnsemble()

def ensemble_decision(svm_pred, svm_conf, kmeans_cluster, kmeans_conf, rule_pred, rule_conf, enhanced_features):
    """
    üß† META-LEARNING ENSEMBLE DECISION
    Uses Neural Network to learn optimal combination instead of hardcoded if-else!
    """
    
    # Collect training sample for continuous learning
    meta_ensemble.collect_training_sample(
        svm_pred, svm_conf, kmeans_cluster, kmeans_conf,
        rule_pred, rule_conf, enhanced_features
    )
    
    # Get AI-powered ensemble prediction
    result = meta_ensemble.predict(
        svm_pred, svm_conf, kmeans_cluster, kmeans_conf,
        rule_pred, rule_conf, enhanced_features
    )
    
    # Auto-train meta-model when enough data is collected
    if len(meta_ensemble.training_data) >= 100 and not meta_ensemble.is_trained:
        logger.info("Auto-training Meta-Learning model...")
        meta_ensemble.train_meta_model()
    
    return result

def ensemble_decision_simplified(kmeans_cluster, kmeans_conf, rule_pred, rule_conf, enhanced_features):
    """
    Simplified ensemble decision for K-Means + Rule-based only
    """
    # Adaptive weights based on confidence
    total_confidence = kmeans_conf + rule_conf
    if total_confidence > 0:
        kmeans_weight = kmeans_conf / total_confidence
        rule_weight = rule_conf / total_confidence
    else:
        kmeans_weight = 0.5
        rule_weight = 0.5
    
    # Soft voting
    makespan_scores = {"small": 0, "medium": 0, "large": 0}
    
    # Rule-based contribution  
    makespan_scores[rule_pred] += rule_weight
    
    # K-Means contribution with soft mapping
    cluster_to_workload = {
        0: {"small": 0.7, "medium": 0.3, "large": 0.0},
        1: {"small": 0.5, "medium": 0.4, "large": 0.1},
        2: {"small": 0.3, "medium": 0.6, "large": 0.1},
        3: {"small": 0.1, "medium": 0.7, "large": 0.2},
        4: {"small": 0.0, "medium": 0.5, "large": 0.5},
        5: {"small": 0.0, "medium": 0.3, "large": 0.7}
    }
    
    cluster_mapping = cluster_to_workload.get(kmeans_cluster, 
                                            {"small": 0.33, "medium": 0.33, "large": 0.33})
    
    for workload, prob in cluster_mapping.items():
        makespan_scores[workload] += kmeans_weight * prob
    
    # Final decision
    final_makespan = max(makespan_scores, key=makespan_scores.get)
    
    # Calculate ensemble confidence
    ensemble_confidence = max(makespan_scores.values()) / sum(makespan_scores.values())
    
    return {
        "makespan": final_makespan,
        "cluster": kmeans_cluster,
        "confidence": ensemble_confidence,
        "weights": {"kmeans": kmeans_weight, "rule": rule_weight},
        "makespan_scores": makespan_scores
    }

@app.route('/admin/metrics', methods=['GET'])
@performance_tracker
def get_metrics():
    """
    üìä ADVANCED METRICS ENDPOINT
    Tr·∫£ v·ªÅ chi ti·∫øt performance metrics cho monitoring
    """
    try:
        # Get system stats
        system_stats = perf_monitor.get_stats()
        
        # Calculate endpoint-specific metrics
        endpoint_metrics = {}
        for endpoint, requests in request_metrics.items():
            if requests:
                response_times = [r['response_time'] for r in requests]
                success_rate = sum(1 for r in requests if r['success']) / len(requests)
                
                endpoint_metrics[endpoint] = {
                    'total_requests': len(requests),
                    'average_response_time_ms': np.mean(response_times) * 1000,
                    'min_response_time_ms': np.min(response_times) * 1000,
                    'max_response_time_ms': np.max(response_times) * 1000,
                    'success_rate': success_rate,
                    'errors': len(requests) - sum(1 for r in requests if r['success'])
                }
        
        # Model performance metrics
        model_stats = {
            'svm': {
                'loaded': svm_model is not None,
                'kernel': svm_model.kernel if svm_model else None,
                'support_vectors': int(sum(svm_model.n_support_)) if svm_model else 0,
                'classes': svm_model.classes_.tolist() if svm_model else []
            },
            'kmeans': {
                'loaded': kmeans_model is not None,
                'clusters': kmeans_model.n_clusters if kmeans_model else 0,
                'inertia': float(kmeans_model.inertia_) if kmeans_model else 0
            }
        }
        
        return jsonify({
            'system': system_stats,
            'cache': {
                'total_entries': len(prediction_cache),
                'hit_rate': performance_stats['cache_hits'] / (performance_stats['cache_hits'] + performance_stats['cache_misses']) if (performance_stats['cache_hits'] + performance_stats['cache_misses']) > 0 else 0,
                'hits': performance_stats['cache_hits'],
                'misses': performance_stats['cache_misses']
            },
            'endpoints': endpoint_metrics,
            'models': model_stats,
            'global_stats': performance_stats,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error in get_metrics: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/admin/debug', methods=['GET'])
@performance_tracker  
def debug_info():
    """
    üîß DEBUG ENDPOINT
    Tr·∫£ v·ªÅ th√¥ng tin debug chi ti·∫øt cho troubleshooting
    """
    try:
        # Recent errors
        recent_errors = []
        for endpoint, requests in request_metrics.items():
            for req in requests[-10:]:  # Last 10 requests
                if not req['success']:
                    recent_errors.append({
                        'endpoint': endpoint,
                        'timestamp': req['timestamp'].isoformat(),
                        'response_time': req['response_time']
                    })
        
        # Cache analysis
        cache_analysis = {
            'total_entries': len(prediction_cache),
            'oldest_entry': min(prediction_cache.values(), key=lambda x: x['timestamp'])['timestamp'].isoformat() if prediction_cache else None,
            'newest_entry': max(prediction_cache.values(), key=lambda x: x['timestamp'])['timestamp'].isoformat() if prediction_cache else None,
            'cache_size_mb': sys.getsizeof(prediction_cache) / 1024 / 1024
        }
        
        # Model detailed info
        model_debug = {}
        if svm_model:
            model_debug['svm'] = {
                'type': str(type(svm_model)),
                'kernel': svm_model.kernel,
                'C': svm_model.C,
                'gamma': svm_model.gamma,
                'n_support': svm_model.n_support_.tolist(),
                'support_vectors_shape': svm_model.support_vectors_.shape,
                'classes': svm_model.classes_.tolist()
            }
        
        if kmeans_model:
            model_debug['kmeans'] = {
                'type': str(type(kmeans_model)),
                'n_clusters': kmeans_model.n_clusters,
                'inertia': float(kmeans_model.inertia_),
                'n_iter': kmeans_model.n_iter_,
                'cluster_centers_shape': kmeans_model.cluster_centers_.shape
            }
        
        return jsonify({
            'service_info': {
                'python_version': sys.version,
                'working_directory': os.getcwd(),
                'environment_variables': dict(os.environ),
                'loaded_modules': list(sys.modules.keys())[:50]  # First 50 modules
            },
            'recent_errors': recent_errors[-20:],  # Last 20 errors
            'cache_analysis': cache_analysis,
            'model_debug': model_debug,
            'memory_usage': {
                'prediction_cache_mb': sys.getsizeof(prediction_cache) / 1024 / 1024,
                'request_metrics_mb': sys.getsizeof(request_metrics) / 1024 / 1024,
                'total_memory_mb': psutil.Process().memory_info().rss / 1024 / 1024
            },
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error in debug_info: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/admin/cache/clear', methods=['POST'])
@performance_tracker
def clear_cache():
    """
    üóëÔ∏è CACHE MANAGEMENT ENDPOINT
    X√≥a cache ƒë·ªÉ refresh predictions
    """
    try:
        cache_size_before = len(prediction_cache)
        prediction_cache.clear()
        
        # Reset cache stats
        performance_stats['cache_hits'] = 0
        performance_stats['cache_misses'] = 0
        
        logger.info(f"Cache cleared: {cache_size_before} entries removed")
        
        return jsonify({
            'status': 'success',
            'message': f'Cache cleared successfully',
            'entries_removed': cache_size_before,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error clearing cache: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/admin/models/reload', methods=['POST'])
@performance_tracker
def reload_models():
    """
    üîÑ MODEL RELOAD ENDPOINT
    Reload models t·ª´ disk (useful for model updates)
    """
    try:
        logger.info("Reloading models...")
        load_models()
        
        # Clear cache after model reload
        prediction_cache.clear()
        
        return jsonify({
            'status': 'success',
            'message': 'Models reloaded successfully',
            'models_loaded': {
                'svm': svm_model is not None,
                'kmeans': kmeans_model is not None,
                'scalers': svm_scaler is not None and kmeans_scaler is not None
            },
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error reloading models: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/predict/compare', methods=['POST'])
@performance_tracker
def compare_predictions():
    """
    üî¨ PREDICTION COMPARISON ENDPOINT
    So s√°nh predictions t·ª´ individual models vs ensemble
    FIXED: Updated to use correct 9-feature SVM model
    """
    try:
        if svm_model is None or kmeans_model is None:
            return jsonify({"error": "Models not loaded"}), 503
        
        data = request.get_json()
        if not data or "features" not in data:
            return jsonify({"error": "Missing 'features' field"}), 400
        
        features = data["features"]
        vm_features = data.get("vm_features", [0.5, 0.5, 0.5])
        
        # Validate features for comparison - SVM model uses 9 features
        if len(features) != 9:
            return jsonify({"error": "Expected 9 features for SVM: [jobs_1min, jobs_5min, jobs_15min, memory_mb, disk_gb, cpu_cores, cpu_speed, network_receive_kbps, network_transmit_kbps]"}), 400
        
        start_time = time.time()
        
        # Individual SVM prediction (9 features)
        features_scaled = svm_scaler.transform([features])
        svm_pred_numeric = svm_model.predict(features_scaled)[0]
        svm_prediction = svm_label_encoder.inverse_transform([svm_pred_numeric])[0]
        svm_decision_scores = svm_model.decision_function(features_scaled)
        svm_confidence = float(np.abs(svm_decision_scores[0])) if not isinstance(svm_decision_scores[0], np.ndarray) else float(np.max(np.abs(svm_decision_scores[0])))
        
        # Individual K-Means prediction (3 VM features)
        vm_scaled = kmeans_scaler.transform([vm_features])
        kmeans_cluster = int(kmeans_model.predict(vm_scaled)[0])
        kmeans_distances = kmeans_model.transform(vm_scaled)[0]
        kmeans_distance = float(np.min(kmeans_distances))
        
        # Simple ensemble decision for comparison
        if svm_confidence > 0.8:
            ensemble_decision = svm_prediction
            ensemble_algorithm = "SVM (High Confidence)"
        elif kmeans_distance < 0.5:
            cluster_to_workload = {0: "small", 1: "small", 2: "medium", 3: "medium", 4: "large", 5: "large"}
            ensemble_decision = cluster_to_workload.get(kmeans_cluster, "medium")
            ensemble_algorithm = "K-Means (Close Cluster)"
        else:
            ensemble_decision = svm_prediction  # SVM fallback
            ensemble_algorithm = "SVM (Fallback)"
        
        processing_time = time.time() - start_time
        
        return jsonify({
            'individual_predictions': {
                'svm': {
                    'prediction': svm_prediction,
                    'confidence': svm_confidence,
                    'decision_scores': svm_decision_scores.tolist() if isinstance(svm_decision_scores[0], np.ndarray) else [float(svm_decision_scores[0])]
                },
                'kmeans': {
                    'cluster': kmeans_cluster,
                    'distance_to_centroid': kmeans_distance,
                    'all_distances': kmeans_distances.tolist()
                }
            },
            'ensemble_prediction': {
                'decision': ensemble_decision,
                'algorithm_used': ensemble_algorithm,
                'confidence': min(svm_confidence / 3.0, 0.95) if svm_confidence > 0.8 else 1.0 - kmeans_distance
            },
            'comparison': {
                'svm_vs_ensemble': svm_prediction == ensemble_decision,
                'processing_time_ms': processing_time * 1000,
                'features_used': features,
                'vm_features_used': vm_features,
                'note': 'SVM model uses 9 features as actually trained'
            },
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error in compare_predictions: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/predict/meta_learning', methods=['POST'])
@performance_tracker
@cache_prediction
def predict_meta_learning():
    """
    üß† META-LEARNING NEURAL NETWORK PREDICTION
    Direct access to the trained 96% accuracy Meta-Learning system
    Input: {"features": [13 combined features from SVM, K-Means, and business logic]}
    Output: {"makespan": "small|medium|large", "confidence": float, "method": "MetaLearning_NeuralNetwork"}
    """
    try:
        if meta_learning_model is None or meta_learning_scaler is None or meta_learning_encoder is None:
            return jsonify({"error": "Meta-Learning model components not loaded"}), 503
        
        data = request.get_json()
        if not data or "features" not in data:
            return jsonify({"error": "Missing 'features' field"}), 400
        
        features = data["features"]
        
        # Validate input - Meta-Learning expects 13 features
        if len(features) != 13:
            return jsonify({
                "error": "Expected 13 features for Meta-Learning model",
                "expected_features": meta_learning_features,
                "received_count": len(features)
            }), 400
        
        # Validate feature ranges based on training data
        try:
            # Scale features using trained scaler
            features_scaled = meta_learning_scaler.transform([features])
            
            # Predict using trained Meta-Learning Neural Network
            prediction_proba = meta_learning_model.predict_proba(features_scaled)[0]
            predicted_class_idx = np.argmax(prediction_proba)
            predicted_class = meta_learning_encoder.inverse_transform([predicted_class_idx])[0]
            
            # Confidence from neural network probabilities
            confidence = float(np.max(prediction_proba))
            
            # Get all class probabilities
            prediction_probabilities = {
                class_name: float(prob) 
                for class_name, prob in zip(
                    meta_learning_encoder.classes_, 
                    prediction_proba
                )
            }
            
            # Feature importance (approximate)
            feature_importance = abs(features_scaled[0])
            feature_importance = feature_importance / np.sum(feature_importance)
            
            # Map features to importance
            feature_contributions = {
                feature_name: float(importance)
                for feature_name, importance in zip(meta_learning_features, feature_importance)
            }
            
            logger.info(f"Meta-Learning prediction: {predicted_class} (confidence: {confidence:.3f})")
            
            return jsonify({
                "makespan": predicted_class,
                "confidence": confidence,
                "method": "MetaLearning_NeuralNetwork",
                "model_info": {
                    "architecture": meta_learning_info.get('architecture', 'Unknown'),
                    "training_accuracy": meta_learning_info.get('training_accuracy', 0),
                    "test_accuracy": meta_learning_info.get('test_accuracy', 0),
                    "cross_val_accuracy": meta_learning_info.get('cross_val_accuracy', 0)
                },
                "prediction_probabilities": prediction_probabilities,
                "feature_contributions": feature_contributions,
                "features_used": features,
                "timestamp": datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"Error in Meta-Learning prediction: {e}")
            return jsonify({"error": f"Prediction failed: {str(e)}"}), 500
        
    except Exception as e:
        logger.error(f"Error in predict_meta_learning: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/predict/mccva_complete', methods=['POST'])
@performance_tracker
@cache_prediction
def predict_mccva_complete():
    """
    üéØ COMPLETE MCCVA 3-STAGE META-LEARNING PREDICTION
    Automated pipeline: Raw Input ‚Üí SVM ‚Üí K-Means ‚Üí Meta-Learning Neural Network
    Input: {"cpu_cores": int, "memory_gb": float, "storage_gb": float, "network_bandwidth": int, "priority": int}
    Output: {"makespan": "small|medium|large", "confidence": float, "stage_results": {...}}
    """
    try:
        if not all([svm_model, kmeans_model, meta_learning_model]):
            return jsonify({"error": "Not all models loaded for complete MCCVA pipeline"}), 503
        
        data = request.get_json()
        if not data:
            return jsonify({"error": "No request data"}), 400
        
        # Extract raw inputs
        cpu_cores = data.get("cpu_cores", 4)
        memory_gb = data.get("memory_gb", 8)
        storage_gb = data.get("storage_gb", 100)
        network_bandwidth = data.get("network_bandwidth", 1000)
        priority = data.get("priority", 3)
        
        # Prepare input feature vectors for each stage
        base_features = np.array([cpu_cores, memory_gb, storage_gb, network_bandwidth, priority]).reshape(1, -1)
        
        # ===== STAGE 1: SVM Classification =====
        # Ensure feature dimensions match what SVM expects
        svm_features = base_features[:, :min(base_features.shape[1], svm_scaler.mean_.shape[0])]
        # Scale features
        svm_features_scaled = svm_scaler.transform(svm_features)
        # Predict with SVM
        svm_prediction = svm_model.predict(svm_features_scaled)[0]
        svm_proba = svm_model.predict_proba(svm_features_scaled)[0]
        svm_confidence = float(np.max(svm_proba))
        
        # ===== STAGE 2: K-Means Clustering =====
        # Ensure feature dimensions match what K-Means expects
        kmeans_features = base_features[:, :min(base_features.shape[1], kmeans_scaler.mean_.shape[0])]
        # Scale features
        kmeans_features_scaled = kmeans_scaler.transform(kmeans_features)
        # Predict cluster
        kmeans_cluster = int(kmeans_model.predict(kmeans_features_scaled)[0])
        kmeans_distances = kmeans_model.transform(kmeans_features_scaled)[0]
        kmeans_confidence = 1.0 / (1.0 + float(kmeans_distances.min()))
        
        # ===== STAGE 3: Meta-Learning Neural Network =====
        # Prepare meta-features for final prediction
        meta_features = np.zeros((1, 13))  # Assuming 13 features as defined earlier
        
        # Base features (first 5)
        meta_features[0, :5] = base_features[0, :5]
        
        # SVM probabilities (3 classes)
        meta_features[0, 5:8] = svm_proba
        
        # KMeans one-hot encoding (5 clusters)
        cluster_idx = kmeans_cluster % 5  # Ensure cluster is in 0-4 range
        meta_features[0, 8 + cluster_idx] = 1.0
        
        # Scale meta-features
        meta_features_scaled = meta_learning_scaler.transform(meta_features)
        
        # Make final prediction with Meta-Learning model
        meta_prediction = meta_learning_model.predict(meta_features_scaled)[0]
        meta_proba = meta_learning_model.predict_proba(meta_features_scaled)[0]
        meta_confidence = float(np.max(meta_proba))
        
        # Prepare final response
        result = {
            "makespan": meta_prediction,
            "confidence": meta_confidence,
            "stage_results": {
                "stage1_svm": {
                    "prediction": svm_prediction,
                    "confidence": svm_confidence,
                    "probabilities": {
                        "small": float(svm_proba[0]),
                        "medium": float(svm_proba[1]),
                        "large": float(svm_proba[2])
                    }
                },
                "stage2_kmeans": {
                    "cluster": kmeans_cluster,
                    "confidence": kmeans_confidence,
                    "distances": [float(d) for d in kmeans_distances]
                },
                "stage3_metalearning": {
                    "prediction": meta_prediction,
                    "confidence": meta_confidence,
                    "probabilities": {
                        "small": float(meta_proba[0]),
                        "medium": float(meta_proba[1]),
                        "large": float(meta_proba[2])
                    }
                }
            },
            "input": {
                "cpu_cores": cpu_cores,
                "memory_gb": memory_gb,
                "storage_gb": storage_gb,
                "network_bandwidth": network_bandwidth,
                "priority": priority
            },
            "timestamp": datetime.now().isoformat()
        }
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"MCCVA complete pipeline error: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/demo/predict', methods=['POST'])
def demo_predict():
    """
    Demo endpoint cho presentation - s·ª≠ d·ª•ng m√¥ h√¨nh th·ª±c t·∫ø
    """
    start_time = time.time()
    
    try:
        # L·∫•y d·ªØ li·ªáu t·ª´ request
        data = request.get_json()
        if not data:
            return jsonify({
                'error': 'No data provided',
                'status': 'error'
            }), 400
        
        # Tr√≠ch xu·∫•t features
        features = {
            'cpu_cores': data.get('cpu_cores', 4),
            'memory_gb': data.get('memory_gb', 8),
            'storage_gb': data.get('storage_gb', 100),
            'network_bandwidth': data.get('network_bandwidth', 1000),
            'priority': data.get('priority', 3),
            'vm_cpu_usage': data.get('vm_cpu_usage', 0.5),
            'vm_memory_usage': data.get('vm_memory_usage', 0.5),
            'vm_storage_usage': data.get('vm_storage_usage', 0.5)
        }
        
        # Chuy·ªÉn ƒë·ªïi features th√†nh m·∫£ng numpy
        feature_array = np.array([
            [
                features['cpu_cores'],
                features['memory_gb'], 
                features['storage_gb'],
                features['network_bandwidth'],
                features['priority'],
                features['vm_cpu_usage'],
                features['vm_memory_usage'],
                features['vm_storage_usage'],
                features['cpu_cores'] * features['vm_cpu_usage']  # Th√™m feature ph√°i sinh
            ]
        ])
        
        # Giai ƒëo·∫°n 1: SVM Prediction
        svm_result = "medium"  # Default
        svm_confidence = 0.5
        
        try:
            if svm_model is not None and svm_scaler is not None:
                # Chu·∫©n h√≥a d·ªØ li·ªáu
                scaled_features = svm_scaler.transform(feature_array)
                
                # D·ª± ƒëo√°n
                svm_result = svm_model.predict(scaled_features)[0]
                
                # L·∫•y confidence
                if hasattr(svm_model, 'decision_function'):
                    svm_confidence = abs(float(svm_model.decision_function(scaled_features)[0]))
                elif hasattr(svm_model, 'predict_proba'):
                    proba = svm_model.predict_proba(scaled_features)[0]
                    svm_confidence = float(np.max(proba))
                else:
                    svm_confidence = 0.7  # Default
        except Exception as e:
            logger.error(f"SVM prediction error: {e}")
            # Fallback to simple rules
            if features['cpu_cores'] <= 2:
                svm_result = "small"
            elif features['cpu_cores'] >= 8:
                svm_result = "large"
            else:
                svm_result = "medium"
        
        # Giai ƒëo·∫°n 2: K-Means Clustering
        kmeans_cluster = 0
        kmeans_confidence = 0.5
        
        try:
            if kmeans_model is not None and kmeans_scaler is not None:
                # Chu·∫©n h√≥a d·ªØ li·ªáu
                scaled_features = kmeans_scaler.transform(feature_array)
                
                # D·ª± ƒëo√°n cluster
                kmeans_cluster = int(kmeans_model.predict(scaled_features)[0])
                
                # T√≠nh kho·∫£ng c√°ch ƒë·∫øn centroid
                distances = kmeans_model.transform(scaled_features)[0]
                min_distance = float(np.min(distances))
                kmeans_confidence = 1.0 / (1.0 + min_distance)  # Chuy·ªÉn ƒë·ªïi kho·∫£ng c√°ch th√†nh confidence
        except Exception as e:
            logger.error(f"K-Means prediction error: {e}")
            # Fallback to simple rules
            if features['memory_gb'] <= 4:
                kmeans_cluster = 0  # Low resource cluster
            elif features['memory_gb'] >= 32:
                kmeans_cluster = 2  # High resource cluster
            else:
                kmeans_cluster = 1  # Medium resource cluster
        
        # Giai ƒëo·∫°n 3: Meta-Learning
        meta_result = svm_result  # Default to SVM result
        meta_confidence = 0.7
        
        try:
            if meta_learning_model is not None and meta_learning_scaler is not None:
                # T·∫°o meta features
                svm_proba = np.zeros(3)  # One-hot for small, medium, large
                if svm_result == "small":
                    svm_proba[0] = svm_confidence
                elif svm_result == "medium":
                    svm_proba[1] = svm_confidence
                else:
                    svm_proba[2] = svm_confidence
                
                # T·∫°o one-hot cho cluster
                cluster_one_hot = np.zeros(6)  # Gi·∫£ s·ª≠ t·ªëi ƒëa 6 clusters
                if 0 <= kmeans_cluster < 6:
                    cluster_one_hot[kmeans_cluster] = 1
                
                # K·∫øt h·ª£p features
                meta_features = np.concatenate([
                    feature_array[0],
                    svm_proba,
                    cluster_one_hot
                ]).reshape(1, -1)
                
                # Chu·∫©n h√≥a
                meta_features_scaled = meta_learning_scaler.transform(meta_features)
                
                # D·ª± ƒëo√°n
                meta_result = meta_learning_model.predict(meta_features_scaled)[0]
                
                # L·∫•y confidence
                if hasattr(meta_learning_model, 'predict_proba'):
                    proba = meta_learning_model.predict_proba(meta_features_scaled)[0]
                    meta_confidence = float(np.max(proba))
        except Exception as e:
            logger.error(f"Meta-Learning prediction error: {e}")
            # Fallback to ensemble logic
            if svm_confidence > kmeans_confidence:
                meta_result = svm_result
                meta_confidence = svm_confidence
            else:
                # Map cluster to workload type
                cluster_to_workload = {
                    0: "small", 1: "small",
                    2: "medium", 3: "medium",
                    4: "large", 5: "large"
                }
                meta_result = cluster_to_workload.get(kmeans_cluster, "medium")
                meta_confidence = kmeans_confidence
        
        # Quy·∫øt ƒë·ªãnh routing
        routing_decision = "vm_pool_" + meta_result
        
        # T·∫°o response
        response = {
            "makespan": meta_result,
            "confidence": meta_confidence,
            "routing_decision": routing_decision,
            "stage_results": {
                "stage1_svm": {
                    "prediction": svm_result,
                    "confidence": svm_confidence
                },
                "stage2_kmeans": {
                    "cluster": kmeans_cluster,
                    "confidence": kmeans_confidence
                },
                "stage3_metalearning": {
                    "prediction": meta_result,
                    "confidence": meta_confidence
                }
            },
            "input_features": features,
            "processing_time_ms": (time.time() - start_time) * 1000
        }
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"Demo prediction error: {e}")
        return jsonify({
            'error': str(e),
            'status': 'error'
        }), 500

if __name__ == '__main__':
    # Load models khi kh·ªüi ƒë·ªông
    load_models()
    
    # Ch·∫°y Flask app
    logger.info("üöÄ ML Service ƒëang kh·ªüi ƒë·ªông tr√™n port 5000...")
    app.run(host='0.0.0.0', port=5000, debug=False) 