#!/usr/bin/env python3
"""
Advanced SVM Training with Full Dataset (train_svm_full.py)
Hu·∫•n luy·ªán SVM v·ªõi to√†n b·ªô dataset t·ª´ th∆∞ m·ª•c dataset/modified
"""

import pandas as pd
import numpy as np
import os
import time
import joblib
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.svm import SVC
from sklearn.utils.class_weight import compute_class_weight
import warnings
warnings.filterwarnings('ignore')

print("üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán SVM v·ªõi to√†n b·ªô dataset...")
start_time = time.time()

# H√†m √°nh x·∫° nh√£n
def map_class_names(class_name):
    mapping = {
        "'Very Low'": 'small',
        "'Low'": 'small',
        "'Medium'": 'medium',
        "'High'": 'large',
        "'Very High'": 'large'
    }
    return mapping.get(class_name, 'medium')

# H√†m hu·∫•n luy·ªán SVM v·ªõi GridSearchCV
def train_optimized_svm(X, y, type_val=None):
    """Hu·∫•n luy·ªán SVM v·ªõi GridSearchCV ƒë·ªÉ t√¨m hyperparameters t·ªëi ∆∞u"""
    print(f"  Hu·∫•n luy·ªán SVM v·ªõi GridSearchCV...")
    
    # Chuy·ªÉn ƒë·ªïi nh√£n chu·ªói th√†nh s·ªë nguy√™n n·∫øu c·∫ßn
    if isinstance(y.iloc[0], str):
        label_encoder = LabelEncoder()
        y_encoded = label_encoder.fit_transform(y)
    else:
        label_encoder = None
        y_encoded = y
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # T√≠nh class weights ƒë·ªÉ x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng d·ªØ li·ªáu
    unique_classes = np.unique(y_encoded)
    class_weights = compute_class_weight(class_weight='balanced', classes=unique_classes, y=y_encoded)
    class_weight_dict = {i: weight for i, weight in zip(unique_classes, class_weights)}
    print(f"  Class weights: {class_weight_dict}")
    
    # Tham s·ªë cho GridSearchCV
    param_grid = {
        'C': [0.1, 1, 10, 100],
        'gamma': ['scale', 'auto', 0.01, 0.1],
        'kernel': ['rbf', 'poly']
    }
    
    # Hu·∫•n luy·ªán SVM v·ªõi GridSearchCV
    grid_search = GridSearchCV(
        SVC(probability=True, random_state=42, class_weight='balanced'),
        param_grid=param_grid,
        cv=5,
        scoring='f1_weighted',
        verbose=0,
        n_jobs=-1
    )
    
    grid_search.fit(X_scaled, y_encoded if label_encoder else y)
    
    # In th√¥ng tin v·ªÅ best parameters
    print(f"  Best parameters: {grid_search.best_params_}")
    print(f"  Best cross-validation score: {grid_search.best_score_:.4f}")
    
    # L·∫•y m√¥ h√¨nh t·ªët nh·∫•t
    best_model = grid_search.best_estimator_
    
    return {
        'model': best_model,
        'scaler': scaler,
        'label_encoder': label_encoder,
        'type': type_val,
        'best_params': grid_search.best_params_,
        'best_score': grid_search.best_score_
    }

# H√†m d·ª± ƒëo√°n v·ªõi m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
def predict_with_model(model_data, X):
    """D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán"""
    # D·ª± ƒëo√°n v·ªõi SVM
    X_scaled = model_data['scaler'].transform(X)
    y_pred = model_data['model'].predict(X_scaled)
    
    # Chuy·ªÉn ƒë·ªïi l·∫°i nh√£n s·ªë th√†nh chu·ªói n·∫øu c·∫ßn
    if model_data['label_encoder'] is not None:
        y_pred = model_data['label_encoder'].inverse_transform(y_pred)
    
    return y_pred

def main():
    """H√†m ch√≠nh ƒë·ªÉ hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh"""
    # T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a c√≥
    os.makedirs("models", exist_ok=True)
    
    # B∆∞·ªõc 1: Load to√†n b·ªô dataset
    print("\nüìä ƒêang load to√†n b·ªô dataset...")
    dataset_dir = "dataset/modified"
    
    all_dfs = []
    for file in os.listdir(dataset_dir):
        if file.endswith(".xlsx"):
            try:
                file_path = os.path.join(dataset_dir, file)
                df = pd.read_excel(file_path)
                all_dfs.append(df)
                print(f"‚úÖ ƒê√£ load {len(df)} m·∫´u t·ª´ {file}")
            except Exception as e:
                print(f"‚ùå L·ªói khi ƒë·ªçc file {file}: {e}")
    
    if not all_dfs:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file dataset n√†o!")
        return
    
    # K·∫øt h·ª£p t·∫•t c·∫£ dataframes
    df = pd.concat(all_dfs, ignore_index=True)
    print(f"\nüìã T·ªïng s·ªë m·∫´u: {len(df)}")
    
    # Hi·ªÉn th·ªã th√¥ng tin v·ªÅ d·ªØ li·ªáu
    print("\nüìã Th√¥ng tin v·ªÅ d·ªØ li·ªáu:")
    print(f"  S·ªë l∆∞·ª£ng m·∫´u: {len(df)}")
    print(f"  S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng: {len(df.columns) - 2}")  # Tr·ª´ Class_Name v√† Type
    
    # Hi·ªÉn th·ªã ph√¢n ph·ªëi nh√£n
    print("\nüìä Ph√¢n ph·ªëi nh√£n:")
    class_counts = df['Class_Name'].value_counts()
    for cls, count in class_counts.items():
        print(f"  {cls}: {count} m·∫´u ({count/len(df)*100:.1f}%)")
    
    # Hi·ªÉn th·ªã ph√¢n ph·ªëi Type
    print("\nüìä Ph√¢n ph·ªëi Type:")
    type_counts = df['Type'].value_counts()
    for typ, count in type_counts.items():
        print(f"  Type {typ}: {count} m·∫´u ({count/len(df)*100:.1f}%)")
    
    # B∆∞·ªõc 2: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
    print("\nüîÑ Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu...")
    
    # √Ånh x·∫° nh√£n
    df['target'] = df['Class_Name'].apply(map_class_names)
    
    # Chuy·ªÉn ƒë·ªïi c√°c c·ªôt th√†nh s·ªë
    print("  Chuy·ªÉn ƒë·ªïi c√°c c·ªôt th√†nh s·ªë...")
    numeric_cols = df.select_dtypes(include=['number']).columns
    X = df[numeric_cols].copy()
    
    # Ki·ªÉm tra v√† x·ª≠ l√Ω gi√° tr·ªã NaN
    nan_count = X.isna().sum().sum()
    if nan_count > 0:
        print(f"  ‚ö†Ô∏è Ph√°t hi·ªán {nan_count} gi√° tr·ªã NaN, ƒëang thay th·∫ø b·∫±ng gi√° tr·ªã trung b√¨nh...")
        from sklearn.impute import SimpleImputer
        imputer = SimpleImputer(strategy='mean')
        X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)
    
    # B∆∞·ªõc 3: Hu·∫•n luy·ªán m√¥ h√¨nh chung cho t·∫•t c·∫£ c√°c Type
    print("\nüß† Hu·∫•n luy·ªán m√¥ h√¨nh chung cho t·∫•t c·∫£ c√°c Type...")
    
    # Chia d·ªØ li·ªáu th√†nh train/test
    y = df['target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Hu·∫•n luy·ªán m√¥ h√¨nh chung
    global_model_data = train_optimized_svm(X_train, y_train)
    
    # ƒê√°nh gi√° m√¥ h√¨nh chung
    y_pred = predict_with_model(global_model_data, X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\nüìà K·∫øt qu·∫£ ƒë√°nh gi√° m√¥ h√¨nh chung:")
    print(f"  Accuracy: {accuracy:.4f}")
    print("\nüìä Classification Report:")
    print(classification_report(y_test, y_pred))
    print("\nüìä Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    
    # B∆∞·ªõc 4: Hu·∫•n luy·ªán m√¥ h√¨nh cho t·ª´ng Type
    print("\nüß† Hu·∫•n luy·ªán m√¥ h√¨nh cho t·ª´ng Type...")
    
    type_models = {}
    for type_val in sorted(df['Type'].unique()):
        print(f"\nüîç Hu·∫•n luy·ªán m√¥ h√¨nh cho Type {type_val}...")
        
        # L·∫•y d·ªØ li·ªáu cho Type c·ª• th·ªÉ
        df_type = df[df['Type'] == type_val]
        X_type = df_type[numeric_cols].copy()
        y_type = df_type['target']
        
        print(f"  Type {type_val} c√≥ {len(df_type)} m·∫´u")
        
        # Hi·ªÉn th·ªã ph√¢n ph·ªëi nh√£n cho Type n√†y
        print(f"  Ph√¢n ph·ªëi nh√£n cho Type {type_val}:")
        type_class_counts = y_type.value_counts()
        for cls, count in type_class_counts.items():
            print(f"    - {cls}: {count} m·∫´u ({count/len(df_type)*100:.1f}%)")
        
        # Ki·ªÉm tra v√† x·ª≠ l√Ω gi√° tr·ªã NaN
        nan_count = X_type.isna().sum().sum()
        if nan_count > 0:
            print(f"  ‚ö†Ô∏è Ph√°t hi·ªán {nan_count} gi√° tr·ªã NaN, ƒëang thay th·∫ø b·∫±ng gi√° tr·ªã trung b√¨nh...")
            from sklearn.impute import SimpleImputer
            imputer = SimpleImputer(strategy='mean')
            X_type = pd.DataFrame(imputer.fit_transform(X_type), columns=X_type.columns)
        
        # Chia d·ªØ li·ªáu th√†nh train/test
        X_type_train, X_type_test, y_type_train, y_type_test = train_test_split(
            X_type, y_type, test_size=0.2, random_state=42)
        
        # Hu·∫•n luy·ªán m√¥ h√¨nh ri√™ng cho Type n√†y
        type_model_data = train_optimized_svm(X_type_train, y_type_train, type_val)
        
        # ƒê√°nh gi√° m√¥ h√¨nh
        y_type_pred = predict_with_model(type_model_data, X_type_test)
        type_accuracy = accuracy_score(y_type_test, y_type_pred)
        print(f"  Accuracy cho Type {type_val}: {type_accuracy:.4f}")
        
        # Hi·ªÉn th·ªã ph√¢n ph·ªëi nh√£n trong t·∫≠p test
        print(f"  Ph√¢n ph·ªëi nh√£n trong t·∫≠p test:")
        for label, count in pd.Series(y_type_test).value_counts().items():
            print(f"    - {label}: {count} m·∫´u")
        
        # Hi·ªÉn th·ªã confusion matrix n·∫øu c√≥ nhi·ªÅu h∆°n 1 l·ªõp
        if len(pd.Series(y_type_test).unique()) > 1:
            print(f"  Confusion Matrix:")
            print(confusion_matrix(y_type_test, y_type_pred))
        
        # L∆∞u m√¥ h√¨nh
        type_models[type_val] = type_model_data
    
    # B∆∞·ªõc 5: L∆∞u c√°c m√¥ h√¨nh
    print("\nüíæ L∆∞u c√°c m√¥ h√¨nh...")
    
    # L∆∞u m√¥ h√¨nh chung
    joblib.dump(global_model_data, "models/svm_global_model_full.joblib")
    
    # L∆∞u m√¥ h√¨nh cho t·ª´ng Type
    joblib.dump(type_models, "models/svm_type_models_full.joblib")
    
    # L∆∞u th√¥ng tin m√¥ h√¨nh
    model_info = {
        'accuracy': float(accuracy),
        'n_features': X.shape[1],
        'n_samples': len(df),
        'n_types': len(df['Type'].unique()),
        'class_distribution': {k: int(v) for k, v in class_counts.items()},
        'type_distribution': {int(k): int(v) for k, v in type_counts.items()},
        'best_params': global_model_data['best_params'],
        'best_score': global_model_data['best_score'],
        'training_time': time.time() - start_time
    }
    joblib.dump(model_info, "models/svm_model_info_full.joblib")
    
    print("\n‚úÖ Hu·∫•n luy·ªán v√† l∆∞u m√¥ h√¨nh th√†nh c√¥ng!")
    print(f"‚è±Ô∏è T·ªïng th·ªùi gian: {(time.time() - start_time):.2f} gi√¢y")

if __name__ == "__main__":
    main() 